{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6b7f42-aee5-4ad0-b568-d05e4f8c09ae",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a67483-71e9-4c96-9085-e2cf6bad1a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/rubchume1/code/Users/rubchume/VoiceCloningFakeAudioDetection'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext extensions\n",
    "%cd_repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e4bc91-8a18-4578-a8f2-5f27b054c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/voicecloningenv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "import directory_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac85ce3e-a9d0-4e23-b414-4e4eded96661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproduce_audio_file_with_pydub(audio_file):\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955c7fd-2ab0-46ea-9353-2fee1f4af759",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781c5a3c-8461-46aa-867b-0c11c34bb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloned_voices_path = \"\"\n",
    "real_voices_path = directory_structure.data_path / \"Common Voice/cv-corpus-15-delta-2023-09-08/en\"\n",
    "real_voices_info_file = real_voices_path / \"validated.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e4afac-2197-4853-9669-af51e51b3d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_info = pd.read_csv(real_voices_info_file, delimiter=\"\\t\")[\"path\"].map(\n",
    "    lambda path: str(real_voices_path / \"clips\" / path)\n",
    ")\n",
    "cloned_info = pd.Series([str(path) for path in Path(\"outputs/OOTB-YourTTS/TIMITexamples/\").glob(\"*.wav\")]).rename(\"path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73845149-bf39-4daa-ace1-b0036d04dfb3",
   "metadata": {},
   "source": [
    "# Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06759e1a-1751-4d47-ab0e-c848a9c1c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "def reproduce_audio_from_pcm_samples(pcm_samples: np.array, sample_rate: int):\n",
    "    audio = Audio(data=pcm_samples, rate=sample_rate, autoplay=True)\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7840079a-799c-4845-8c56-9fb2ed0790ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioBinaryDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        negative_audio_files: Iterable,\n",
    "        postive_audio_files: Iterable,\n",
    "        target_sample_rate: int,\n",
    "        num_samples: int,\n",
    "        max_imbalance=1,\n",
    "        random_seed=0,\n",
    "    ):\n",
    "        self.negative_audio_files = list(negative_audio_files)\n",
    "        self.positive_audio_files = list(postive_audio_files)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.random_instance = random.Random(random_seed)\n",
    "        \n",
    "        negative_samples, positive_samples = self._undersample_unbalanced_dataset(\n",
    "            self.negative_audio_files,\n",
    "            self.positive_audio_files,\n",
    "            max_imbalance\n",
    "        )\n",
    "        \n",
    "        negative_samples_with_label = [\n",
    "            (sample, 0)\n",
    "            for sample in negative_samples\n",
    "        ]\n",
    "        \n",
    "        positive_samples_with_label = [\n",
    "            (sample, 1)\n",
    "            for sample in negative_samples\n",
    "        ]\n",
    "        \n",
    "        self.samples = self.random_instance.sample(\n",
    "            negative_samples_with_label + positive_samples_with_label,\n",
    "            len(negative_samples_with_label) + len(positive_samples_with_label)\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_file, label = self.samples[index]\n",
    "        audio_segment = AudioSegment.from_file(audio_file)\n",
    "        audio_resampled = audio_segment.set_frame_rate(self.target_sample_rate)\n",
    "        pcm_samples = self._bytes_to_numpy(\n",
    "            audio_resampled.raw_data,\n",
    "            audio_resampled.sample_width\n",
    "        )\n",
    "        resized_samples = np.zeros(self.num_samples)\n",
    "        resized_samples[:len(pcm_samples)] = pcm_samples[:self.num_samples]\n",
    "        return torch.Tensor(resized_samples), label\n",
    "    \n",
    "    def _undersample_unbalanced_dataset(self, dataset_A: List, dataset_B: List, max_imbalance):\n",
    "        if len(dataset_A) > len(dataset_B):\n",
    "            dataset_big = dataset_A\n",
    "            dataset_small = dataset_B\n",
    "            a_bigger_than_b = True\n",
    "        else:\n",
    "            dataset_big = dataset_B\n",
    "            dataset_small = dataset_A\n",
    "            a_bigger_than_b = True\n",
    "        \n",
    "        if max_imbalance < 1:\n",
    "            max_imbalance = 1 / max_imbalance\n",
    "            \n",
    "        max_samples = int(len(dataset_small) * max_imbalance)\n",
    "        samples_big = self.random_instance.sample(dataset_big, min(max_samples, len(dataset_big)))\n",
    "        samples_small = self.random_instance.sample(dataset_small, len(dataset_small))\n",
    "        \n",
    "        if a_bigger_than_b:\n",
    "            return samples_big, samples_small\n",
    "        else:\n",
    "            return samples_small, samples_big\n",
    "    \n",
    "    @staticmethod\n",
    "    def _bytes_to_numpy(bytes_stream: bytes, sample_width=2) -> np.array:\n",
    "        \"\"\"\n",
    "        sample_width: number of bytes per sample\n",
    "        \"\"\"\n",
    "        dtype_map = {\n",
    "            1: np.int8,\n",
    "            2: np.int16,\n",
    "            4: np.int32\n",
    "        }\n",
    "\n",
    "        if sample_width not in dtype_map:\n",
    "            raise ValueError(f\"Unsupported sample width: {sample_width}\")\n",
    "\n",
    "        return np.frombuffer(bytes_stream, dtype=dtype_map[sample_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c99d64-2ff7-4e29-9c07-df6d685ef700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, target_sample_rate, num_samples, cloned_samples: pd.Series, real_samples: pd.Series):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.cloned_samples = cloned_samples\n",
    "        self.real_samples = real_samples\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        dataset = AudioBinaryDataset(\n",
    "            self.real_samples,\n",
    "            self.cloned_samples,\n",
    "            self.target_sample_rate,\n",
    "            self.num_samples\n",
    "        )\n",
    "        \n",
    "        self.dataset_training, self.dataset_validation, self.dataset_test = random_split(\n",
    "            dataset,\n",
    "            [0.7, 0.1, 0.2],\n",
    "            generator=torch.Generator().manual_seed(0)\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_training, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_validation, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31a0ec25-dc10-41f1-a607-c6699b0095fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "class Stage(Enum):\n",
    "    TRAIN = \"TRAIN\"\n",
    "    VALIDATION = \"VALIDATION\"\n",
    "    TEST = \"TEST\"\n",
    "\n",
    "\n",
    "class ClonedAudioDetector(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._create_model()\n",
    "        self._prepare_metrics()\n",
    "        self.debug = False\n",
    "        \n",
    "    def _create_model(self):\n",
    "        num_labels = 2\n",
    "\n",
    "        label2id = dict(\n",
    "            cloned=1,\n",
    "            real=0)\n",
    "\n",
    "        id2label = {\n",
    "            1: \"cloned\",\n",
    "            0: \"real\"\n",
    "        }\n",
    "\n",
    "        self.model = AutoModelForAudioClassification.from_pretrained(\n",
    "            \"facebook/wav2vec2-base\",\n",
    "            num_labels=num_labels,\n",
    "            label2id=label2id,\n",
    "            id2label=id2label\n",
    "        )\n",
    "             \n",
    "    def _prepare_metrics(self):\n",
    "        self.precision = torchmetrics.Precision(task='binary')\n",
    "        self.recall = torchmetrics.Recall(task='binary')\n",
    "        self.f1 = torchmetrics.F1Score(task='binary')\n",
    "        self.confmat = torchmetrics.ConfusionMatrix(task=\"binary\")\n",
    "\n",
    "        self.targets_scores = {}\n",
    "        self.targets_predicted = {}\n",
    "        self.targets = {}\n",
    "        \n",
    "        self._reset_target_registries(Stage.TRAIN)\n",
    "        self._reset_target_registries(Stage.VALIDATION)\n",
    "        self._reset_target_registries(Stage.TEST)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "    \n",
    "    def criterion(self, logits, labels):\n",
    "        return nn.functional.cross_entropy(logits, labels)\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        return self._step(batch, Stage.TRAIN)\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        return self._step(batch, Stage.VALIDATION)\n",
    "        \n",
    "    def test_step(self, batch, batch_index):\n",
    "        return self._step(batch, Stage.TEST)\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        self._reset_target_registries(Stage.TRAIN)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self._log_epoch_metrics(Stage.TRAIN)\n",
    "    \n",
    "    def on_validation_epoch_start(self):\n",
    "        self._reset_target_registries(Stage.VALIDATION)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self._log_epoch_metrics(Stage.VALIDATION)\n",
    "    \n",
    "    def on_test_epoch_start(self):\n",
    "        self._reset_target_registries(Stage.TEST)\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        self._log_epoch_metrics(Stage.TEST)\n",
    "                                \n",
    "    def _reset_target_registries(self, stage: Stage):\n",
    "        self.targets_scores[stage] = []\n",
    "        self.targets_predicted[stage] = []\n",
    "        self.targets[stage] = []\n",
    "\n",
    "    def _step(self, batch, stage: Stage):\n",
    "        audios, targets = batch\n",
    "        logits, targets_predicted = self._predict(audios)\n",
    "        if self.debug:\n",
    "            import pdb; pdb.set_trace()\n",
    "        self.targets_scores[stage].append(logits)\n",
    "        self.targets_predicted[stage].append(targets_predicted)\n",
    "        self.targets[stage].append(targets)\n",
    "        \n",
    "        loss = self.criterion(logits, targets)\n",
    "        \n",
    "        metric_name = {\n",
    "            stage.TRAIN: \"train_loss\",\n",
    "            stage.VALIDATION: \"val_loss\",\n",
    "            stage.TEST: \"test_loss\",\n",
    "        }\n",
    "        \n",
    "        self.log(metric_name[stage], loss, prog_bar=True)\n",
    "        return loss\n",
    "        \n",
    "    def _predict(self, data):\n",
    "        logits = self.forward(data).logits\n",
    "        if self.debug:\n",
    "            import pdb; pdb.set_trace()\n",
    "        if torch.any(torch.isnan(logits)):\n",
    "            import pdb; pdb.set_trace()\n",
    "        targets_predicted = (logits[:, 1] > logits[:, 0]) * 1\n",
    "        return logits, targets_predicted\n",
    "        \n",
    "    def _log_epoch_metrics(self, stage: Stage):\n",
    "        targets_predicted = torch.cat(self.targets_predicted[stage], dim=0).squeeze()\n",
    "        targets = torch.cat(self.targets[stage], dim=0)\n",
    "\n",
    "        precision = self.precision(targets_predicted, targets)\n",
    "        recall = self.recall(targets_predicted, targets)\n",
    "        f1_score = self.f1(targets_predicted, targets)\n",
    "\n",
    "        self.log(f'{stage.value}_precision', precision, prog_bar=True)\n",
    "        self.log(f'{stage.value}_recall', recall, prog_bar=True)\n",
    "        self.log(f'{stage.value}_f1', f1_score, prog_bar=True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def get_targets(self, stage: Stage):\n",
    "        return torch.cat(self.targets[stage], dim=0).to(torch.device(\"cpu\"))\n",
    "    \n",
    "    def get_targets_scores(self, stage: Stage):\n",
    "        if self.debug:\n",
    "            import pdb; pdb.set_trace()\n",
    "        return torch.cat(self.targets_scores[stage], dim=0).squeeze().to(torch.device(\"cpu\"))\n",
    "    \n",
    "    def get_targets_predicted(self, stage: Stage):\n",
    "        if self.debug:\n",
    "            import pdb; pdb.set_trace()\n",
    "        return torch.cat(self.targets_predicted[stage], 0).squeeze().to(torch.device(\"cpu\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b2ea1f6-a0f8-4db3-9d9b-f16733599ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/voicecloningenv/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type                              | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model     | Wav2Vec2ForSequenceClassification | 94.6 M\n",
      "1 | precision | BinaryPrecision                   | 0     \n",
      "2 | recall    | BinaryRecall                      | 0     \n",
      "3 | f1        | BinaryF1Score                     | 0     \n",
      "4 | confmat   | BinaryConfusionMatrix             | 0     \n",
      "----------------------------------------------------------------\n",
      "94.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "94.6 M    Total params\n",
      "378.276   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ae80e6d75e40db8605ccb6e30e283e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cd2bd0718c45178009c80b24465f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          TEST_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      TEST_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        TEST_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         TEST_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     TEST_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       TEST_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': nan, 'TEST_precision': 0.0, 'TEST_recall': 0.0, 'TEST_f1': 0.0}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "data_module = DataModule(4, 16000, 64000, real_info, cloned_info)\n",
    "\n",
    "logger = TensorBoardLogger(str(directory_structure.training_artifacts_path), name=\"wav2vec2\")\n",
    "detector = ClonedAudioDetector()\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    max_epochs=3,\n",
    "    accelerator=\"auto\",\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[],\n",
    "    limit_train_batches=10,\n",
    "    # limit_val_batches=5,\n",
    ")\n",
    "\n",
    "trainer.fit(detector, data_module)\n",
    "trainer.test(detector, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1697fa18-92cf-42ca-a1d5-ac5e364b0fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_module.train_dataloader()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c20dde84-d23f-4c0e-95ab-e19f918b6932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.model(next(iter(data_module.train_dataloader()))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d0516fb-d907-4044-a4a5-4c3fe747a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/voicecloningenv/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "label2id = dict(\n",
    "            cloned=1,\n",
    "            real=0)\n",
    "\n",
    "id2label = {\n",
    "    1: \"cloned\",\n",
    "    0: \"real\"\n",
    "}\n",
    "    \n",
    "modelraw = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\",\n",
    "    num_labels=2,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d025379-77ce-4a2a-9235-b5df5dd53be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0192,  0.0464],\n",
       "        [-0.0110,  0.0339],\n",
       "        [-0.0276,  0.0096],\n",
       "        [-0.0205,  0.0120]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelraw(next(iter(data_module.train_dataloader()))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb9d65dc-23af-4f7b-82e4-011700b41bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(directory_structure.models_path / \"wav2vec2.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed1452-89e4-4a5d-b655-e19a030d1e96",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16df2e74-12a1-4671-8119-5bf77d14eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "prediction_metric_functions = dict(\n",
    "    recall=torchmetrics.functional.recall,\n",
    "    precision=torchmetrics.functional.precision,\n",
    "    f1=torchmetrics.functional.f1_score,\n",
    "    acc=torchmetrics.functional.accuracy,\n",
    ")\n",
    "\n",
    "\n",
    "score_metric_functions = dict(\n",
    "    roc_auc=torchmetrics.AUROC(task=\"binary\"),\n",
    ")\n",
    "\n",
    "\n",
    "def get_metrics(model, stage: Stage, threshold=0.5) -> pd.Series:\n",
    "    targets = model.get_targets(stage)\n",
    "    targets_scores = model.get_targets_scores(stage)\n",
    "    targets_predicted = model.get_targets_predicted(stage)\n",
    "    \n",
    "    prediction_metrics = {\n",
    "        metric: function(preds=targets_predicted, target=targets, task=\"binary\", threshold=threshold)\n",
    "        for metric, function in prediction_metric_functions.items()\n",
    "    }\n",
    "    \n",
    "    import pdb; pdb.set_trace()\n",
    "    score_metrics = {\n",
    "        metric: function(targets_scores, targets)\n",
    "        for metric, function in score_metric_functions.items()\n",
    "    }\n",
    "    \n",
    "    return pd.Series(prediction_metrics | score_metrics)\n",
    "\n",
    "\n",
    "def plot_metrics(model, stage: Stage, threshold=0.5):\n",
    "    metrics = get_metrics(model, stage, threshold)\n",
    "    return go.Figure(\n",
    "        data=go.Bar(x=metrics.index, y=metrics),\n",
    "        layout_title=f\"Threshold: {threshold}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, stage: Stage, threshold=0.5):\n",
    "    targets = model.get_targets(stage)\n",
    "    targets_scores = model.get_targets_scores(stage)\n",
    "    # targets_predicted = targets_scores > 0#model.get_targets_predicted(stage)\n",
    "    cm = torchmetrics.ConfusionMatrix(task=\"binary\", threshold=threshold)(targets_scores, targets).numpy()\n",
    "    \n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "\n",
    "\n",
    "def draw_roc(model, stage: Stage, threshold=0.5):\n",
    "    targets = model.get_targets(stage)\n",
    "    targets_scores = model.get_targets_scores(stage)\n",
    "    \n",
    "    fpr, tpr, thresholds = torchmetrics.ROC(task=\"binary\", threshold=threshold)(targets_scores, targets)\n",
    "    \n",
    "    index = np.argmin(abs(thresholds - threshold))\n",
    "    \n",
    "    return go.Figure(\n",
    "        data=[\n",
    "            go.Scatter(x=fpr, y=tpr),\n",
    "            go.Scatter(x=[fpr[index]], y=[tpr[index]], showlegend=False, mode=\"markers+text\", text=f\"Threshold = {threshold}\", textposition=\"middle right\")\n",
    "        ],\n",
    "        layout=dict(\n",
    "            height=500,\n",
    "            width=500,\n",
    "            xaxis_title=\"False positive rate\",\n",
    "            yaxis_title=\"True positive rate\",\n",
    "            title=\"ROC\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a438fd04-0c82-4760-85ce-05eec4e2599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/voicecloningenv/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.bias', 'projector.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClonedAudioDetector(\n",
       "  (model): Wav2Vec2ForSequenceClassification(\n",
       "    (wav2vec2): Wav2Vec2Model(\n",
       "      (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Wav2Vec2GroupNormConvLayer(\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "            (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feature_projection): Wav2Vec2FeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): Wav2Vec2Encoder(\n",
       "        (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): Wav2Vec2SamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "            (attention): Wav2Vec2Attention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): Wav2Vec2FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projector): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       "  (precision): BinaryPrecision()\n",
       "  (recall): BinaryRecall()\n",
       "  (f1): BinaryF1Score()\n",
       "  (confmat): BinaryConfusionMatrix()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = ClonedAudioDetector.load_from_checkpoint(checkpoint_path=directory_structure.models_path / \"wav2vec2.ckpt\")\n",
    "detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fd6a374-6d10-4e81-ad10-1d43483d51b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d13e0069d1a4a3ba94bcda492955ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4183/1450390607.py\u001b[0m(114)\u001b[0;36m_predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    112 \u001b[0;31m        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    113 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 114 \u001b[0;31m        \u001b[0mtargets_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    115 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    116 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  logits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0.,    0.,    0.,  ...,  234.,  441.,  659.],\n",
      "        [   0.,    0.,    0.,  ...,    4.,    4.,    4.],\n",
      "        [   0.,    0.,    0.,  ...,  185., -114.,  -90.],\n",
      "        [   0.,    0.,    0.,  ...,    0.,  -12.,  -16.]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  data.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64000])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.forward(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  torch.isnan(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  any(torch.isnan(data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RuntimeError: Boolean value of Tensor with more than one value is ambiguous\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  torch.any(torch.isnan(data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "data_module = DataModule(4, 16000, 64000, real_info, cloned_info)\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "# trainer.validate(detector, data_module)\n",
    "trainer.test(detector, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d01fa33-05f8-4304-b48c-bc24ca82b175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4183/3190872002.py\u001b[0m(137)\u001b[0;36mget_targets_scores\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    135 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mget_targets_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    136 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 137 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    138 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    139 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mget_targets_predicted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  torch.cat(self.targets_scores[stage], dim=0).squeeze().to(torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.targets_scores[stage]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]]), tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]])]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "detector.get_targets_scores(Stage.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf33db0-4573-4599-aff8-3f51b68f26d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4183/294121932.py\u001b[0m(28)\u001b[0;36mget_metrics\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     26 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 28 \u001b[0;31m    score_metrics = {\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m        \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_metric_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  targets_scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "plot_metrics(detector, Stage.TEST).show()\n",
    "draw_roc(detector, Stage.TEST).show()\n",
    "plot_confusion_matrix(detector, Stage.TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicecloningenv",
   "language": "python",
   "name": "voicecloningenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
