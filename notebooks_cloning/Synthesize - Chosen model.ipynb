{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c2c5ea-047a-409e-ae01-42405ff2711b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af29da69-d485-498a-a745-21c1ec678bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/rubchume1/code/Users/rubchume/VoiceCloningFakeAudioDetection'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext extensions\n",
    "%cd_repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b441e019-6137-4332-8f2a-65f4ba57c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import pandera as pa\n",
    "from pydub import AudioSegment\n",
    "from torchmetrics.text import WordErrorRate\n",
    "from TTS.api import TTS\n",
    "import whisper\n",
    "\n",
    "from notebooks.common_imports import *\n",
    "from utilities import ModelConfigToUpdate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f2d46-0e1f-452d-a7f0-a5be37c473fe",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961957cf-e134-4ba6-90ec-f6d4b6070993",
   "metadata": {},
   "source": [
    "Utility functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a5b1d88-ab85-43cc-823d-534bf6d050bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceVoicePairsSchema(pa.DataFrameModel):\n",
    "    sentence: pa.typing.Series[str]\n",
    "    voice_sample: pa.typing.Series[str]\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class SentenceVoicePairsDataset:\n",
    "    name: str\n",
    "    pairs: pa.typing.DataFrame[SentenceVoicePairsSchema]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        SentenceVoicePairsSchema.validate(self.pairs)\n",
    "\n",
    "        \n",
    "class CapturePrint:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = self._captured_output = StringIO()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.captured_output = self._captured_output.getvalue()\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        \n",
    "        \n",
    "def split_train_test(df: pd.DataFrame, test_proportion):\n",
    "    train_df = df.sample(frac=1 - test_proportion, random_state=42)\n",
    "    test_df = df.drop(train_df.index)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def convert_mp3_to_wav(source_path, dest_path):\n",
    "    audio = AudioSegment.from_mp3(source_path)\n",
    "    audio.export(dest_path, format=\"wav\")\n",
    "\n",
    "\n",
    "class TTSModel(ABC):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    @abstractmethod\n",
    "    def clone_voice(self, sentence, voice_reference, output, language=\"en\"):\n",
    "        \"\"\"Generate audio file in 'output' that speaks the words in 'sentence' with the voice in 'voice_reference'\"\"\"\n",
    "        \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def languages(self):\n",
    "        \"\"\"Return the languages accepted by the model\"\"\"\n",
    "\n",
    "\n",
    "def sythesize_voices(model: TTSModel, sentence_voice_pairs_dataset: SentenceVoicePairsDataset):\n",
    "    voice_cloning_output_folder = directory_structure.audio_output_path / model.name / sentence_voice_pairs_dataset.name\n",
    "    shutil.rmtree(voice_cloning_output_folder, ignore_errors=True)\n",
    "    voice_cloning_output_folder.mkdir(parents=True)\n",
    "    for index, (sentence, voice_reference) in tqdm(\n",
    "        sentence_voice_pairs_dataset.pairs.reset_index(drop=True).iterrows(),\n",
    "        total=len(sentence_voice_pairs_dataset)\n",
    "    ):\n",
    "        output = voice_cloning_output_folder / f\"{index}.wav\"\n",
    "        model.clone_voice(sentence, voice_reference, output)\n",
    "        \n",
    "    index_file_path = voice_cloning_output_folder / \"index.csv\"\n",
    "    sentence_voice_pairs_dataset.pairs.to_csv(index_file_path, index=False)\n",
    "    print(f\"Index file path: {index_file_path}\")\n",
    "    return index_file_path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03065dcd-93be-405e-a072-19291bbb9f56",
   "metadata": {},
   "source": [
    "# Create sentence-voice pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16866251-f99a-439d-8f75-ba23b579450a",
   "metadata": {},
   "source": [
    "Create sentences and voice reference pairs. Sentences are just sequences of words. Voice references refers to audio files that contains the voice we will clone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f3e8d-9810-4a59-b45c-61003ee4f473",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Common Voice inspired examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b6471-cef8-4620-9b0d-f00cfd7fabd2",
   "metadata": {},
   "source": [
    "Get sentences and voices from the Common Voice dataset.\n",
    "This is the dataset you will use for the classification model as real voice samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36dec2-5158-41de-bec9-ddfebc45ba35",
   "metadata": {},
   "source": [
    "Choose sentences to synthesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ff23f67-118a-49fd-82fb-a573473182aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_recordings = 10\n",
    "common_voice_data_path = directory_structure.data_path / \"Common Voice/cv-corpus-15-delta-2023-09-08/en\"\n",
    "validated_recordings = pd.read_csv(common_voice_data_path / \"validated.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c418372a-2fe1-4a3b-8c89-0d4db3d8c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_folder = common_voice_data_path / \"clips\"\n",
    "wav_folder = common_voice_data_path / \"clips_wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "993eefa7-8d06-479a-8bc9-d020a7901818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_common_voice_sentence_voice_pairs():\n",
    "    shutil.rmtree(wav_folder)\n",
    "    recordings_sample = validated_recordings[[\"path\", \"sentence\"]].sample(num_recordings)\n",
    "    wav_folder.mkdir(exist_ok=True)\n",
    "    for recording_name in tqdm(recordings_sample.path):\n",
    "        mp3_path = mp3_folder / recording_name\n",
    "        wav_path = (wav_folder / recording_name).with_suffix(\".wav\")\n",
    "        convert_mp3_to_wav(mp3_path, wav_path)\n",
    "    return recordings_sample\n",
    "        \n",
    "\n",
    "recreate = False\n",
    "if not recreate and wav_folder.is_dir():\n",
    "    paths = [\n",
    "        wav_recording.with_suffix(\".mp3\").name\n",
    "        for wav_recording in wav_folder.iterdir()\n",
    "    ]\n",
    "\n",
    "    recordings_sample = validated_recordings.loc[validated_recordings.path.isin(paths), [\"path\", \"sentence\"]]\n",
    "else:\n",
    "    recordings_sample = create_common_voice_sentence_voice_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f33c6050-fc60-4718-8969-79f136956cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_pairs = pd.concat([\n",
    "    recordings_sample.sentence,\n",
    "    recordings_sample.path.map(lambda recording_name: str((wav_folder / recording_name).with_suffix(\".wav\"))).rename(\"voice_sample\")\n",
    "], axis=\"columns\")\n",
    "\n",
    "common_voice_dataset = SentenceVoicePairsDataset(\n",
    "    name=\"CommonVoice\",\n",
    "    pairs=common_voice_pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f5bb5-006e-4b97-b6a8-c874da6cadd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TIMIT voices and Common Voice sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4fcd71c-9710-473a-bdb6-9645478573d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_local_path = \"/home/azureuser/TIMIT\"\n",
    "\n",
    "wav_files_parameters = pd.DataFrame([\n",
    "    wav_file.parts[-4:]\n",
    "    for wav_file in Path(timit_local_path).glob(\"**/*.WAV\")\n",
    "], columns=[\"Dataset\", \"Dialect\", \"Speaker\", \"Sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7eaba4-8fa0-41b9-98e2-e6712b44bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timit_recording_parameters_to_path(dataset, dialect, speaker, sentence):\n",
    "    return str(Path(timit_local_path) / dataset / dialect / speaker / sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "688dc202-bbc5-4d1c-b0ba-3e60bc6f9728",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validated_recordings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      2\u001b[0m voices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m     wav_files_parameters\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39msample(num_samples, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m s: timit_recording_parameters_to_path(\u001b[38;5;241m*\u001b[39ms\u001b[38;5;241m.\u001b[39mtolist()), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mvalidated_recordings\u001b[49m\u001b[38;5;241m.\u001b[39msentence\u001b[38;5;241m.\u001b[39msample(num_samples, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validated_recordings' is not defined"
     ]
    }
   ],
   "source": [
    "num_samples = 20\n",
    "voices = (\n",
    "    wav_files_parameters\n",
    "    .sample(num_samples, replace=True, ignore_index=True)\n",
    "    .apply(lambda s: timit_recording_parameters_to_path(*s.tolist()), axis=\"columns\")\n",
    ")\n",
    "sentences = validated_recordings.sentence.sample(num_samples, replace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d104d89-0d71-484a-b091-474f6848259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.concat([\n",
    "    sentences.rename(\"sentence\"),\n",
    "    voices.rename(\"voice_sample\")\n",
    "], axis=\"columns\")\n",
    "\n",
    "timit_voice_common_sentence_dataset = SentenceVoicePairsDataset(\n",
    "    name=\"TIMITvoiceCommonSentence\",\n",
    "    pairs=pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30cc96-3862-4700-bbf3-5eb12d1806bf",
   "metadata": {},
   "source": [
    "## TIMIT examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "111c5383-a72b-4d16-9196-dcf151d4086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_local_path = \"/home/azureuser/TIMIT\"\n",
    "\n",
    "wav_files_parameters = pd.DataFrame([\n",
    "    wav_file.parts[-4:]\n",
    "    for wav_file in Path(timit_local_path).glob(\"**/*.WAV\")\n",
    "], columns=[\"Dataset\", \"Dialect\", \"Speaker\", \"Sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33899db2-631c-4922-bf4d-e0feaccd656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timit_recording_parameters_to_wav_path(dataset, dialect, speaker, sentence):\n",
    "    nist_file = Path(timit_local_path) / dataset / dialect / speaker / sentence\n",
    "    wav_file = nist_file.with_name(f\"{nist_file.stem}_converted.wav\")\n",
    "    return str(wav_file)\n",
    "\n",
    "def timit_recording_parameters_to_sentence(dataset, dialect, speaker, sentence):\n",
    "    text_path = (Path(timit_local_path) / dataset / dialect / speaker / sentence).with_suffix(\".TXT\")\n",
    "    original_sentence = text_path.read_text()\n",
    "    sentence = re.search(r\"\\d+ \\d+ (?P<sentence>.*)\", original_sentence).group(\"sentence\")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fbfa6fc5-cfff-43c8-9c9c-68e2c6ad90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "chosen_samples = wav_files_parameters.sample(num_samples, random_state=42)\n",
    "voices = chosen_samples.apply(lambda s: timit_recording_parameters_to_wav_path(*s.tolist()), axis=\"columns\")\n",
    "sentences = chosen_samples.apply(lambda s: timit_recording_parameters_to_sentence(*s.tolist()), axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b164f5ad-8b97-460e-bd88-51afe4d1a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.concat([\n",
    "    sentences.rename(\"sentence\"),\n",
    "    voices.rename(\"voice_sample\")\n",
    "], axis=\"columns\")\n",
    "\n",
    "timit_dataset = SentenceVoicePairsDataset(\n",
    "    name=\"TIMITexamples\",\n",
    "    pairs=pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1df45-af78-4a44-90a3-9ae82fe9c43d",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea17c2-47d5-4164-b4df-9916d43e3c39",
   "metadata": {},
   "source": [
    "## Out-of-the-box YourTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "67ad98cb-0b37-4cb2-bb66-ac5ce7428f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourTTSAPI(TTSModel):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        \n",
    "        with CapturePrint() as cp:\n",
    "            self.tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False)\n",
    "        self.initialization_log = cp.captured_output\n",
    "        \n",
    "    def clone_voice(self, sentence, voice_reference, output, language=\"en\"):\n",
    "        with CapturePrint() as cp:\n",
    "            self.tts.tts_to_file(\n",
    "                sentence,\n",
    "                speaker_wav=voice_reference,\n",
    "                file_path=output,\n",
    "                language=language\n",
    "            )\n",
    "        \n",
    "    @property\n",
    "    def languages(self):\n",
    "        return self.tts.languages\n",
    "    \n",
    "\n",
    "out_of_the_box_yourtts_model = YourTTSAPI(\"OOTB-YourTTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4dbef1-d477-4508-b4e2-b377d5b633d2",
   "metadata": {},
   "source": [
    "# Synthesize voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d0502c7-b62a-4ba4-b2a9-2ed8f935697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d97aba848f3472ebafe8175f6fbabda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index file path: outputs/OOTB-YourTTS/TIMITexamples/index.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('outputs/OOTB-YourTTS/TIMITexamples/index.csv')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_path = sythesize_voices(out_of_the_box_yourtts_model, timit_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3bd2f-95c9-4c12-a8e1-b682292cbe83",
   "metadata": {},
   "source": [
    "# Evaluation of syntesized voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4a84b863-7398-43a9-a392-1fc3639d8cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c25ec2ed79403781f16afa558ba2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transcript_audios(audios_directory: str):\n",
    "    audio_files = list(Path(audios_directory).glob(\"*.wav\"))\n",
    "    model = whisper.load_model(\"base\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        texts = {\n",
    "            int(segment.stem): model.transcribe(str(segment), language=\"en\")[\"text\"].strip()\n",
    "            for segment in tqdm(audio_files)\n",
    "        }\n",
    "        \n",
    "    return texts\n",
    "\n",
    "\n",
    "transcripts = transcript_audios(index_path.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37cf1e-3e88-44a1-9d02-84b1e02277db",
   "metadata": {},
   "source": [
    "sentences = pd.read_csv(index_path).sentence\n",
    "transcripts = pd.Series(transcripts).reindex_like(sentences)\n",
    "WordErrorRate()(transcripts, sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicecloningenv",
   "language": "python",
   "name": "voicecloningenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
